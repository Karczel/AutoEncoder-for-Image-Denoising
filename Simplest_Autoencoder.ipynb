{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot_model autoencoder\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(autoencoder, to_file='Simplest_Autoencoder/autoencoder.png',dpi=80, show_shapes=True)\n",
    "plot_model(encoder, to_file='Simplest_Autoencoder/encoder.png',dpi=80, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.2795\n",
      "Epoch 1: val_loss improved from inf to 0.19011, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2776 - val_loss: 0.1901\n",
      "Epoch 2/50\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.1716\n",
      "Epoch 2: val_loss improved from 0.19011 to 0.15289, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1707 - val_loss: 0.1529\n",
      "Epoch 3/50\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.1441\n",
      "Epoch 3: val_loss improved from 0.15289 to 0.13307, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1435 - val_loss: 0.1331\n",
      "Epoch 4/50\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.1284\n",
      "Epoch 4: val_loss improved from 0.13307 to 0.12123, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1281 - val_loss: 0.1212\n",
      "Epoch 5/50\n",
      "213/235 [==========================>...] - ETA: 0s - loss: 0.1184\n",
      "Epoch 5: val_loss improved from 0.12123 to 0.11270, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1181 - val_loss: 0.1127\n",
      "Epoch 6/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.1113\n",
      "Epoch 6: val_loss improved from 0.11270 to 0.10702, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1111 - val_loss: 0.1070\n",
      "Epoch 7/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.1063\n",
      "Epoch 7: val_loss improved from 0.10702 to 0.10273, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1061 - val_loss: 0.1027\n",
      "Epoch 8/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.1025\n",
      "Epoch 8: val_loss improved from 0.10273 to 0.09955, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1023 - val_loss: 0.0995\n",
      "Epoch 9/50\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0996\n",
      "Epoch 9: val_loss improved from 0.09955 to 0.09718, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0996 - val_loss: 0.0972\n",
      "Epoch 10/50\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0977\n",
      "Epoch 10: val_loss improved from 0.09718 to 0.09558, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0976 - val_loss: 0.0956\n",
      "Epoch 11/50\n",
      "216/235 [==========================>...] - ETA: 0s - loss: 0.0964\n",
      "Epoch 11: val_loss improved from 0.09558 to 0.09453, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0963 - val_loss: 0.0945\n",
      "Epoch 12/50\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0954\n",
      "Epoch 12: val_loss improved from 0.09453 to 0.09382, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0954 - val_loss: 0.0938\n",
      "Epoch 13/50\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0949\n",
      "Epoch 13: val_loss improved from 0.09382 to 0.09328, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0949 - val_loss: 0.0933\n",
      "Epoch 14/50\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0945\n",
      "Epoch 14: val_loss improved from 0.09328 to 0.09300, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0945 - val_loss: 0.0930\n",
      "Epoch 15/50\n",
      "211/235 [=========================>....] - ETA: 0s - loss: 0.0942\n",
      "Epoch 15: val_loss improved from 0.09300 to 0.09278, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0942 - val_loss: 0.0928\n",
      "Epoch 16/50\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.0940\n",
      "Epoch 16: val_loss improved from 0.09278 to 0.09259, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0940 - val_loss: 0.0926\n",
      "Epoch 17/50\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0938\n",
      "Epoch 17: val_loss improved from 0.09259 to 0.09246, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0938 - val_loss: 0.0925\n",
      "Epoch 18/50\n",
      "216/235 [==========================>...] - ETA: 0s - loss: 0.0937\n",
      "Epoch 18: val_loss improved from 0.09246 to 0.09239, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0937 - val_loss: 0.0924\n",
      "Epoch 19/50\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0936\n",
      "Epoch 19: val_loss improved from 0.09239 to 0.09221, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0936 - val_loss: 0.0922\n",
      "Epoch 20/50\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0935\n",
      "Epoch 20: val_loss improved from 0.09221 to 0.09217, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0935 - val_loss: 0.0922\n",
      "Epoch 21/50\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0934\n",
      "Epoch 21: val_loss did not improve from 0.09217\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0934 - val_loss: 0.0922\n",
      "Epoch 22/50\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0933\n",
      "Epoch 22: val_loss improved from 0.09217 to 0.09206, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0933 - val_loss: 0.0921\n",
      "Epoch 23/50\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0933\n",
      "Epoch 23: val_loss improved from 0.09206 to 0.09204, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0933 - val_loss: 0.0920\n",
      "Epoch 24/50\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0932\n",
      "Epoch 24: val_loss improved from 0.09204 to 0.09201, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 25/50\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0932\n",
      "Epoch 25: val_loss did not improve from 0.09201\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 26/50\n",
      "216/235 [==========================>...] - ETA: 0s - loss: 0.0931\n",
      "Epoch 26: val_loss improved from 0.09201 to 0.09197, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 27/50\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.0931\n",
      "Epoch 27: val_loss did not improve from 0.09197\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0931 - val_loss: 0.0920\n",
      "Epoch 28/50\n",
      "217/235 [==========================>...] - ETA: 0s - loss: 0.0931\n",
      "Epoch 28: val_loss improved from 0.09197 to 0.09185, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 29/50\n",
      "217/235 [==========================>...] - ETA: 0s - loss: 0.0930\n",
      "Epoch 29: val_loss improved from 0.09185 to 0.09181, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0931 - val_loss: 0.0918\n",
      "Epoch 30/50\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0930\n",
      "Epoch 30: val_loss did not improve from 0.09181\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 31/50\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0930\n",
      "Epoch 31: val_loss improved from 0.09181 to 0.09174, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0917\n",
      "Epoch 32/50\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0930\n",
      "Epoch 32: val_loss improved from 0.09174 to 0.09173, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0917\n",
      "Epoch 33/50\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0929\n",
      "Epoch 33: val_loss did not improve from 0.09173\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 34/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.0929\n",
      "Epoch 34: val_loss improved from 0.09173 to 0.09172, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 35/50\n",
      "216/235 [==========================>...] - ETA: 0s - loss: 0.0929\n",
      "Epoch 35: val_loss improved from 0.09172 to 0.09170, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 36/50\n",
      "216/235 [==========================>...] - ETA: 0s - loss: 0.0929\n",
      "Epoch 36: val_loss did not improve from 0.09170\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 37/50\n",
      "216/235 [==========================>...] - ETA: 0s - loss: 0.0929\n",
      "Epoch 37: val_loss did not improve from 0.09170\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 38/50\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0928\n",
      "Epoch 38: val_loss improved from 0.09170 to 0.09165, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 39/50\n",
      "217/235 [==========================>...] - ETA: 0s - loss: 0.0928\n",
      "Epoch 39: val_loss did not improve from 0.09165\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 40/50\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0929\n",
      "Epoch 40: val_loss did not improve from 0.09165\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 41/50\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0928\n",
      "Epoch 41: val_loss did not improve from 0.09165\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 42/50\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0928\n",
      "Epoch 42: val_loss improved from 0.09165 to 0.09162, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 43/50\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0928\n",
      "Epoch 43: val_loss improved from 0.09162 to 0.09161, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 44/50\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0928\n",
      "Epoch 44: val_loss did not improve from 0.09161\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0916\n",
      "Epoch 45/50\n",
      "214/235 [==========================>...] - ETA: 0s - loss: 0.0928\n",
      "Epoch 45: val_loss did not improve from 0.09161\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 46/50\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0927\n",
      "Epoch 46: val_loss did not improve from 0.09161\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 47/50\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0927\n",
      "Epoch 47: val_loss improved from 0.09161 to 0.09157, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 48/50\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0927\n",
      "Epoch 48: val_loss did not improve from 0.09157\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0917\n",
      "Epoch 49/50\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0927\n",
      "Epoch 49: val_loss did not improve from 0.09157\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 50/50\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0927\n",
      "Epoch 50: val_loss improved from 0.09157 to 0.09149, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0915\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_autoencoder\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best autoencoder model\n",
    "autoencoder = keras.models.load_model(\"best_autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 696us/step\n",
      "313/313 [==============================] - 0s 762us/step\n"
     ]
    }
   ],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a sparsity constraint on the encoded representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# Add a Dense layer with a L1 activity regularizer\n",
    "encoded = layers.Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
