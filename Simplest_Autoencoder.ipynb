{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# This is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# This is our input image\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot_model autoencoder\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(autoencoder, to_file='autoencoder.png',dpi=80, show_shapes=True)\n",
    "plot_model(encoder, to_file='encoder.png',dpi=80, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our encoded (32-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.2809\n",
      "Epoch 1: val_loss improved from inf to 0.18771, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2751 - val_loss: 0.1877\n",
      "Epoch 2/50\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.1711\n",
      "Epoch 2: val_loss improved from 0.18771 to 0.15343, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1700 - val_loss: 0.1534\n",
      "Epoch 3/50\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.1445\n",
      "Epoch 3: val_loss improved from 0.15343 to 0.13379, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1444 - val_loss: 0.1338\n",
      "Epoch 4/50\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.1289\n",
      "Epoch 4: val_loss improved from 0.13379 to 0.12134, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1285 - val_loss: 0.1213\n",
      "Epoch 5/50\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.1185\n",
      "Epoch 5: val_loss improved from 0.12134 to 0.11332, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1185 - val_loss: 0.1133\n",
      "Epoch 6/50\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.1118\n",
      "Epoch 6: val_loss improved from 0.11332 to 0.10743, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1116 - val_loss: 0.1074\n",
      "Epoch 7/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.1065\n",
      "Epoch 7: val_loss improved from 0.10743 to 0.10293, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1064 - val_loss: 0.1029\n",
      "Epoch 8/50\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.1024\n",
      "Epoch 8: val_loss improved from 0.10293 to 0.09943, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.1023 - val_loss: 0.0994\n",
      "Epoch 9/50\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 9: val_loss improved from 0.09943 to 0.09694, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0994 - val_loss: 0.0969\n",
      "Epoch 10/50\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0974\n",
      "Epoch 10: val_loss improved from 0.09694 to 0.09529, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0974 - val_loss: 0.0953\n",
      "Epoch 11/50\n",
      "212/235 [==========================>...] - ETA: 0s - loss: 0.0960\n",
      "Epoch 11: val_loss improved from 0.09529 to 0.09432, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0961 - val_loss: 0.0943\n",
      "Epoch 12/50\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0952\n",
      "Epoch 12: val_loss improved from 0.09432 to 0.09367, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0952 - val_loss: 0.0937\n",
      "Epoch 13/50\n",
      "212/235 [==========================>...] - ETA: 0s - loss: 0.0947\n",
      "Epoch 13: val_loss improved from 0.09367 to 0.09321, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0947 - val_loss: 0.0932\n",
      "Epoch 14/50\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0943\n",
      "Epoch 14: val_loss improved from 0.09321 to 0.09292, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0943 - val_loss: 0.0929\n",
      "Epoch 15/50\n",
      "214/235 [==========================>...] - ETA: 0s - loss: 0.0941\n",
      "Epoch 15: val_loss improved from 0.09292 to 0.09262, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0940 - val_loss: 0.0926\n",
      "Epoch 16/50\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0939\n",
      "Epoch 16: val_loss improved from 0.09262 to 0.09246, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0938 - val_loss: 0.0925\n",
      "Epoch 17/50\n",
      "217/235 [==========================>...] - ETA: 0s - loss: 0.0937\n",
      "Epoch 17: val_loss improved from 0.09246 to 0.09243, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0937 - val_loss: 0.0924\n",
      "Epoch 18/50\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0936\n",
      "Epoch 18: val_loss improved from 0.09243 to 0.09234, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0936 - val_loss: 0.0923\n",
      "Epoch 19/50\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0934\n",
      "Epoch 19: val_loss improved from 0.09234 to 0.09228, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0934 - val_loss: 0.0923\n",
      "Epoch 20/50\n",
      "212/235 [==========================>...] - ETA: 0s - loss: 0.0934\n",
      "Epoch 20: val_loss improved from 0.09228 to 0.09211, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0934 - val_loss: 0.0921\n",
      "Epoch 21/50\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0933\n",
      "Epoch 21: val_loss did not improve from 0.09211\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0933 - val_loss: 0.0921\n",
      "Epoch 22/50\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0932\n",
      "Epoch 22: val_loss improved from 0.09211 to 0.09202, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 23/50\n",
      "216/235 [==========================>...] - ETA: 0s - loss: 0.0932\n",
      "Epoch 23: val_loss improved from 0.09202 to 0.09196, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0932 - val_loss: 0.0920\n",
      "Epoch 24/50\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0931\n",
      "Epoch 24: val_loss improved from 0.09196 to 0.09185, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 25/50\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.0931\n",
      "Epoch 25: val_loss did not improve from 0.09185\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0931 - val_loss: 0.0919\n",
      "Epoch 26/50\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0930\n",
      "Epoch 26: val_loss did not improve from 0.09185\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0920\n",
      "Epoch 27/50\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.0930\n",
      "Epoch 27: val_loss improved from 0.09185 to 0.09182, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0918\n",
      "Epoch 28/50\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0930\n",
      "Epoch 28: val_loss did not improve from 0.09182\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0930 - val_loss: 0.0919\n",
      "Epoch 29/50\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.0930\n",
      "Epoch 29: val_loss improved from 0.09182 to 0.09181, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 30/50\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0929\n",
      "Epoch 30: val_loss improved from 0.09181 to 0.09176, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0918\n",
      "Epoch 31/50\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0929\n",
      "Epoch 31: val_loss improved from 0.09176 to 0.09174, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 32/50\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0929\n",
      "Epoch 32: val_loss improved from 0.09174 to 0.09168, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0929 - val_loss: 0.0917\n",
      "Epoch 33/50\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0928\n",
      "Epoch 33: val_loss did not improve from 0.09168\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 34/50\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0928\n",
      "Epoch 34: val_loss did not improve from 0.09168\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0918\n",
      "Epoch 35/50\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0928\n",
      "Epoch 35: val_loss improved from 0.09168 to 0.09167, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 36/50\n",
      "217/235 [==========================>...] - ETA: 0s - loss: 0.0928\n",
      "Epoch 36: val_loss did not improve from 0.09167\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 37/50\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0928\n",
      "Epoch 37: val_loss did not improve from 0.09167\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0928 - val_loss: 0.0917\n",
      "Epoch 38/50\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0927\n",
      "Epoch 38: val_loss improved from 0.09167 to 0.09163, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 39/50\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0927\n",
      "Epoch 39: val_loss did not improve from 0.09163\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 40/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.0927\n",
      "Epoch 40: val_loss improved from 0.09163 to 0.09162, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 41/50\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0927\n",
      "Epoch 41: val_loss improved from 0.09162 to 0.09160, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 42/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.0927\n",
      "Epoch 42: val_loss improved from 0.09160 to 0.09155, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0927 - val_loss: 0.0915\n",
      "Epoch 43/50\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0927\n",
      "Epoch 43: val_loss did not improve from 0.09155\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 44/50\n",
      "215/235 [==========================>...] - ETA: 0s - loss: 0.0926\n",
      "Epoch 44: val_loss did not improve from 0.09155\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0927 - val_loss: 0.0916\n",
      "Epoch 45/50\n",
      "212/235 [==========================>...] - ETA: 0s - loss: 0.0926\n",
      "Epoch 45: val_loss did not improve from 0.09155\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.0926 - val_loss: 0.0916\n",
      "Epoch 46/50\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0926\n",
      "Epoch 46: val_loss improved from 0.09155 to 0.09155, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0915\n",
      "Epoch 47/50\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0926\n",
      "Epoch 47: val_loss improved from 0.09155 to 0.09151, saving model to best_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0915\n",
      "Epoch 48/50\n",
      "214/235 [==========================>...] - ETA: 0s - loss: 0.0927\n",
      "Epoch 48: val_loss did not improve from 0.09151\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0916\n",
      "Epoch 49/50\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0926\n",
      "Epoch 49: val_loss did not improve from 0.09151\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0916\n",
      "Epoch 50/50\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0926\n",
      "Epoch 50: val_loss did not improve from 0.09151\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0926 - val_loss: 0.0915\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_autoencoder\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best autoencoder model\n",
    "autoencoder = keras.models.load_model(\"best_autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 446us/step\n",
      "313/313 [==============================] - 0s 431us/step\n"
     ]
    }
   ],
   "source": [
    "# Encode and decode some digits\n",
    "# Note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a sparsity constraint on the encoded representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "\n",
    "input_img = keras.Input(shape=(784,))\n",
    "# Add a Dense layer with a L1 activity regularizer\n",
    "encoded = layers.Dense(encoding_dim, activation='relu',\n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_img)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
