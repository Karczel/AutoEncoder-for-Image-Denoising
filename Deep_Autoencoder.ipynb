{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.utils import plot_model\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = keras.Input(shape=(784,))\n",
    "encoded = layers.Dense(128, activation='relu')(input_img)\n",
    "encoded = layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = layers.Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.1587\n",
      "Epoch 1: val_loss improved from inf to 0.14214, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 2s 5ms/step - loss: 0.1584 - val_loss: 0.1421\n",
      "Epoch 2/100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.1346\n",
      "Epoch 2: val_loss improved from 0.14214 to 0.12607, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1342 - val_loss: 0.1261\n",
      "Epoch 3/100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.1231\n",
      "Epoch 3: val_loss improved from 0.12607 to 0.11733, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1230 - val_loss: 0.1173\n",
      "Epoch 4/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.1165\n",
      "Epoch 4: val_loss improved from 0.11733 to 0.11219, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1164 - val_loss: 0.1122\n",
      "Epoch 5/100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.1120\n",
      "Epoch 5: val_loss improved from 0.11219 to 0.10914, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1120 - val_loss: 0.1091\n",
      "Epoch 6/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.1094\n",
      "Epoch 6: val_loss improved from 0.10914 to 0.10688, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1094 - val_loss: 0.1069\n",
      "Epoch 7/100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.1073\n",
      "Epoch 7: val_loss improved from 0.10688 to 0.10505, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1073 - val_loss: 0.1051\n",
      "Epoch 8/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.1056\n",
      "Epoch 8: val_loss improved from 0.10505 to 0.10345, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1056 - val_loss: 0.1035\n",
      "Epoch 9/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.1040\n",
      "Epoch 9: val_loss improved from 0.10345 to 0.10218, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1040 - val_loss: 0.1022\n",
      "Epoch 10/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.1026\n",
      "Epoch 10: val_loss improved from 0.10218 to 0.10078, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1026 - val_loss: 0.1008\n",
      "Epoch 11/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.1013\n",
      "Epoch 11: val_loss improved from 0.10078 to 0.10010, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1013 - val_loss: 0.1001\n",
      "Epoch 12/100\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.1002\n",
      "Epoch 12: val_loss improved from 0.10010 to 0.09887, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1002 - val_loss: 0.0989\n",
      "Epoch 13/100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0991\n",
      "Epoch 13: val_loss improved from 0.09887 to 0.09762, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0991 - val_loss: 0.0976\n",
      "Epoch 14/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0981\n",
      "Epoch 14: val_loss improved from 0.09762 to 0.09664, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0980 - val_loss: 0.0966\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 15: val_loss improved from 0.09664 to 0.09563, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0970 - val_loss: 0.0956\n",
      "Epoch 16/100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0963\n",
      "Epoch 16: val_loss improved from 0.09563 to 0.09480, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0963 - val_loss: 0.0948\n",
      "Epoch 17/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0956\n",
      "Epoch 17: val_loss improved from 0.09480 to 0.09442, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0956 - val_loss: 0.0944\n",
      "Epoch 18/100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0950\n",
      "Epoch 18: val_loss improved from 0.09442 to 0.09385, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0950 - val_loss: 0.0938\n",
      "Epoch 19/100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0945\n",
      "Epoch 19: val_loss improved from 0.09385 to 0.09335, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0945 - val_loss: 0.0934\n",
      "Epoch 20/100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0941\n",
      "Epoch 20: val_loss improved from 0.09335 to 0.09322, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0941 - val_loss: 0.0932\n",
      "Epoch 21/100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0937\n",
      "Epoch 21: val_loss improved from 0.09322 to 0.09262, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0937 - val_loss: 0.0926\n",
      "Epoch 22/100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0933\n",
      "Epoch 22: val_loss did not improve from 0.09262\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0933 - val_loss: 0.0927\n",
      "Epoch 23/100\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0929\n",
      "Epoch 23: val_loss improved from 0.09262 to 0.09203, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0929 - val_loss: 0.0920\n",
      "Epoch 24/100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0925\n",
      "Epoch 24: val_loss improved from 0.09203 to 0.09130, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0924 - val_loss: 0.0913\n",
      "Epoch 25/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0921\n",
      "Epoch 25: val_loss did not improve from 0.09130\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0921 - val_loss: 0.0913\n",
      "Epoch 26/100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0917\n",
      "Epoch 26: val_loss improved from 0.09130 to 0.09085, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0917 - val_loss: 0.0909\n",
      "Epoch 27/100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0914\n",
      "Epoch 27: val_loss improved from 0.09085 to 0.09056, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0914 - val_loss: 0.0906\n",
      "Epoch 28/100\n",
      "217/235 [==========================>...] - ETA: 0s - loss: 0.0910\n",
      "Epoch 28: val_loss improved from 0.09056 to 0.09009, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0910 - val_loss: 0.0901\n",
      "Epoch 29/100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0908\n",
      "Epoch 29: val_loss did not improve from 0.09009\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0908 - val_loss: 0.0902\n",
      "Epoch 30/100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0906\n",
      "Epoch 30: val_loss improved from 0.09009 to 0.08957, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0905 - val_loss: 0.0896\n",
      "Epoch 31/100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0903\n",
      "Epoch 31: val_loss improved from 0.08957 to 0.08955, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0903 - val_loss: 0.0895\n",
      "Epoch 32/100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0900\n",
      "Epoch 32: val_loss improved from 0.08955 to 0.08934, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0900 - val_loss: 0.0893\n",
      "Epoch 33/100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0899\n",
      "Epoch 33: val_loss improved from 0.08934 to 0.08899, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0899 - val_loss: 0.0890\n",
      "Epoch 34/100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0897\n",
      "Epoch 34: val_loss did not improve from 0.08899\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0897 - val_loss: 0.0890\n",
      "Epoch 35/100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0894\n",
      "Epoch 35: val_loss improved from 0.08899 to 0.08868, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0894 - val_loss: 0.0887\n",
      "Epoch 36/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0893\n",
      "Epoch 36: val_loss did not improve from 0.08868\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0893 - val_loss: 0.0887\n",
      "Epoch 37/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0891\n",
      "Epoch 37: val_loss improved from 0.08868 to 0.08850, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0891 - val_loss: 0.0885\n",
      "Epoch 38/100\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0890\n",
      "Epoch 38: val_loss did not improve from 0.08850\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0890 - val_loss: 0.0888\n",
      "Epoch 39/100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0888\n",
      "Epoch 39: val_loss improved from 0.08850 to 0.08821, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0888 - val_loss: 0.0882\n",
      "Epoch 40/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0887\n",
      "Epoch 40: val_loss improved from 0.08821 to 0.08792, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0887 - val_loss: 0.0879\n",
      "Epoch 41/100\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0885\n",
      "Epoch 41: val_loss did not improve from 0.08792\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0886 - val_loss: 0.0880\n",
      "Epoch 42/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0884\n",
      "Epoch 42: val_loss improved from 0.08792 to 0.08789, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0884 - val_loss: 0.0879\n",
      "Epoch 43/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0883\n",
      "Epoch 43: val_loss improved from 0.08789 to 0.08775, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0883 - val_loss: 0.0878\n",
      "Epoch 44/100\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.0882\n",
      "Epoch 44: val_loss did not improve from 0.08775\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0882 - val_loss: 0.0879\n",
      "Epoch 45/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0881\n",
      "Epoch 45: val_loss improved from 0.08775 to 0.08737, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0881 - val_loss: 0.0874\n",
      "Epoch 46/100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0880\n",
      "Epoch 46: val_loss did not improve from 0.08737\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0880 - val_loss: 0.0874\n",
      "Epoch 47/100\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0879\n",
      "Epoch 47: val_loss improved from 0.08737 to 0.08729, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0879 - val_loss: 0.0873\n",
      "Epoch 48/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0878\n",
      "Epoch 48: val_loss did not improve from 0.08729\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0878 - val_loss: 0.0874\n",
      "Epoch 49/100\n",
      "220/235 [===========================>..] - ETA: 0s - loss: 0.0877\n",
      "Epoch 49: val_loss improved from 0.08729 to 0.08700, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0877 - val_loss: 0.0870\n",
      "Epoch 50/100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0876\n",
      "Epoch 50: val_loss improved from 0.08700 to 0.08690, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0876 - val_loss: 0.0869\n",
      "Epoch 51/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0875\n",
      "Epoch 51: val_loss improved from 0.08690 to 0.08683, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0875 - val_loss: 0.0868\n",
      "Epoch 52/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0874\n",
      "Epoch 52: val_loss improved from 0.08683 to 0.08683, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0874 - val_loss: 0.0868\n",
      "Epoch 53/100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0873\n",
      "Epoch 53: val_loss improved from 0.08683 to 0.08680, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0873 - val_loss: 0.0868\n",
      "Epoch 54/100\n",
      "217/235 [==========================>...] - ETA: 0s - loss: 0.0874\n",
      "Epoch 54: val_loss improved from 0.08680 to 0.08669, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0873 - val_loss: 0.0867\n",
      "Epoch 55/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0872\n",
      "Epoch 55: val_loss did not improve from 0.08669\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0872 - val_loss: 0.0869\n",
      "Epoch 56/100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0871\n",
      "Epoch 56: val_loss did not improve from 0.08669\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0871 - val_loss: 0.0868\n",
      "Epoch 57/100\n",
      "219/235 [==========================>...] - ETA: 0s - loss: 0.0870\n",
      "Epoch 57: val_loss improved from 0.08669 to 0.08661, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0870 - val_loss: 0.0866\n",
      "Epoch 58/100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0869\n",
      "Epoch 58: val_loss improved from 0.08661 to 0.08657, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0870 - val_loss: 0.0866\n",
      "Epoch 59/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0869\n",
      "Epoch 59: val_loss improved from 0.08657 to 0.08648, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0869 - val_loss: 0.0865\n",
      "Epoch 60/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0868\n",
      "Epoch 60: val_loss improved from 0.08648 to 0.08637, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0869 - val_loss: 0.0864\n",
      "Epoch 61/100\n",
      "214/235 [==========================>...] - ETA: 0s - loss: 0.0867\n",
      "Epoch 61: val_loss improved from 0.08637 to 0.08622, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0867 - val_loss: 0.0862\n",
      "Epoch 62/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0867\n",
      "Epoch 62: val_loss did not improve from 0.08622\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0867 - val_loss: 0.0863\n",
      "Epoch 63/100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0866\n",
      "Epoch 63: val_loss improved from 0.08622 to 0.08616, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0866 - val_loss: 0.0862\n",
      "Epoch 64/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0866\n",
      "Epoch 64: val_loss did not improve from 0.08616\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0866 - val_loss: 0.0862\n",
      "Epoch 65/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0865\n",
      "Epoch 65: val_loss did not improve from 0.08616\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0865 - val_loss: 0.0862\n",
      "Epoch 66/100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0865\n",
      "Epoch 66: val_loss improved from 0.08616 to 0.08615, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0865 - val_loss: 0.0862\n",
      "Epoch 67/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0864\n",
      "Epoch 67: val_loss did not improve from 0.08615\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0864 - val_loss: 0.0864\n",
      "Epoch 68/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0864\n",
      "Epoch 68: val_loss improved from 0.08615 to 0.08600, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0864 - val_loss: 0.0860\n",
      "Epoch 69/100\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0863\n",
      "Epoch 69: val_loss improved from 0.08600 to 0.08593, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0863 - val_loss: 0.0859\n",
      "Epoch 70/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0863\n",
      "Epoch 70: val_loss did not improve from 0.08593\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0863 - val_loss: 0.0860\n",
      "Epoch 71/100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0863\n",
      "Epoch 71: val_loss did not improve from 0.08593\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0863 - val_loss: 0.0860\n",
      "Epoch 72/100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0862\n",
      "Epoch 72: val_loss improved from 0.08593 to 0.08577, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0862 - val_loss: 0.0858\n",
      "Epoch 73/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0861\n",
      "Epoch 73: val_loss improved from 0.08577 to 0.08576, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0861 - val_loss: 0.0858\n",
      "Epoch 74/100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0861\n",
      "Epoch 74: val_loss improved from 0.08576 to 0.08569, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0861 - val_loss: 0.0857\n",
      "Epoch 75/100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0861\n",
      "Epoch 75: val_loss improved from 0.08569 to 0.08567, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0861 - val_loss: 0.0857\n",
      "Epoch 76/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0860\n",
      "Epoch 76: val_loss improved from 0.08567 to 0.08566, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0860 - val_loss: 0.0857\n",
      "Epoch 77/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0859\n",
      "Epoch 77: val_loss improved from 0.08566 to 0.08564, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0859 - val_loss: 0.0856\n",
      "Epoch 78/100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0859\n",
      "Epoch 78: val_loss did not improve from 0.08564\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0859 - val_loss: 0.0860\n",
      "Epoch 79/100\n",
      "218/235 [==========================>...] - ETA: 0s - loss: 0.0859\n",
      "Epoch 79: val_loss improved from 0.08564 to 0.08549, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0859 - val_loss: 0.0855\n",
      "Epoch 80/100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0858\n",
      "Epoch 80: val_loss improved from 0.08549 to 0.08537, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0858 - val_loss: 0.0854\n",
      "Epoch 81/100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0858\n",
      "Epoch 81: val_loss did not improve from 0.08537\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0858 - val_loss: 0.0858\n",
      "Epoch 82/100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0858\n",
      "Epoch 82: val_loss did not improve from 0.08537\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0858 - val_loss: 0.0854\n",
      "Epoch 83/100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0857\n",
      "Epoch 83: val_loss did not improve from 0.08537\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0857 - val_loss: 0.0858\n",
      "Epoch 84/100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0857\n",
      "Epoch 84: val_loss did not improve from 0.08537\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0857 - val_loss: 0.0859\n",
      "Epoch 85/100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0857\n",
      "Epoch 85: val_loss improved from 0.08537 to 0.08531, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0857 - val_loss: 0.0853\n",
      "Epoch 86/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0856\n",
      "Epoch 86: val_loss did not improve from 0.08531\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0856 - val_loss: 0.0855\n",
      "Epoch 87/100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0857\n",
      "Epoch 87: val_loss did not improve from 0.08531\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0857 - val_loss: 0.0855\n",
      "Epoch 88/100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0856\n",
      "Epoch 88: val_loss improved from 0.08531 to 0.08526, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0856 - val_loss: 0.0853\n",
      "Epoch 89/100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0856\n",
      "Epoch 89: val_loss did not improve from 0.08526\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0856 - val_loss: 0.0854\n",
      "Epoch 90/100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0855\n",
      "Epoch 90: val_loss did not improve from 0.08526\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0855 - val_loss: 0.0855\n",
      "Epoch 91/100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0855\n",
      "Epoch 91: val_loss did not improve from 0.08526\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0855 - val_loss: 0.0854\n",
      "Epoch 92/100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0854\n",
      "Epoch 92: val_loss improved from 0.08526 to 0.08502, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0855 - val_loss: 0.0850\n",
      "Epoch 93/100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0854\n",
      "Epoch 93: val_loss did not improve from 0.08502\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0854 - val_loss: 0.0852\n",
      "Epoch 94/100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0854\n",
      "Epoch 94: val_loss did not improve from 0.08502\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0854 - val_loss: 0.0852\n",
      "Epoch 95/100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0854\n",
      "Epoch 95: val_loss did not improve from 0.08502\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0853 - val_loss: 0.0851\n",
      "Epoch 96/100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0853\n",
      "Epoch 96: val_loss did not improve from 0.08502\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0853 - val_loss: 0.0852\n",
      "Epoch 97/100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0854\n",
      "Epoch 97: val_loss did not improve from 0.08502\n",
      "235/235 [==============================] - 1s 3ms/step - loss: 0.0854 - val_loss: 0.0850\n",
      "Epoch 98/100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0853\n",
      "Epoch 98: val_loss improved from 0.08502 to 0.08490, saving model to best_deep_autoencoder\n",
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deep_autoencoder\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0853 - val_loss: 0.0849\n",
      "Epoch 99/100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0852\n",
      "Epoch 99: val_loss did not improve from 0.08490\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0852 - val_loss: 0.0851\n",
      "Epoch 100/100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0852\n",
      "Epoch 100: val_loss did not improve from 0.08490\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0852 - val_loss: 0.0849\n"
     ]
    }
   ],
   "source": [
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_deep_autoencoder\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot_model autoencoder\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(autoencoder, to_file='Deep_Autoencoder/autoencoder.png',dpi=80, show_shapes=True)\n",
    "plot_model(encoder, to_file='Deep_Autoencoder/encoder.png',dpi=80, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 660us/step\n"
     ]
    }
   ],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
